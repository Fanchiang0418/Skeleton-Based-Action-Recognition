import cv2
import mediapipe as mp
import numpy as np
import tensorflow as tf
import json
import os
import requests

# âœ… LSTM æ¨¡å‹
model = tf.keras.models.load_model("lstm_action_model.h5")

# âœ… å‹•ä½œæ¨™ç±¤
with open("action_dict.json", "r") as f:
    action_dict = json.load(f)
reverse_dict = {v: k for k, v in action_dict.items()}

# âœ… MediaPipe
mp_pose = mp.solutions.pose
pose = mp_pose.Pose(static_image_mode=False, min_detection_confidence=0.3, min_tracking_confidence=0.3)

time_steps = 30
video_folder = "data"

# âœ… æœ¬åœ°æ¨¡æ¿ï¼ˆFallbackï¼‰
local_templates = {
    "jump": "èˆè€…è¼•ç›ˆåœ°è·³èºï¼Œå®›å¦‚é¢¨ä¸­çš„ç¾½æ¯›ã€‚",
    "wave": "èˆè€…æ®å‹•é›™æ‰‹ï¼Œå¦‚æ°´æ³¢èˆ¬æŸ”ç¾ã€‚",
    "run": "èˆè€…ç–¾é€Ÿå¥”è·‘ï¼Œæ•£ç™¼å¼·çƒˆçš„ç”Ÿå‘½åŠ›ã€‚"
}

# âœ… å‘¼å« Ollamaï¼ˆæ”¯æŒ qwen/llama2/mistralï¼‰
def ollama_generate(prompt, model_name="qwen"):
    try:
        url = "http://localhost:11434/api/generate"
        headers = {"Content-Type": "application/json"}
        payload = {"model": model_name, "prompt": prompt, "stream": False}
        r = requests.post(url, headers=headers, json=payload, timeout=30)
        if r.status_code == 200:
            return r.json().get("response", "").strip()
    except Exception as e:
        print(f"âš ï¸ Ollama ç”Ÿæˆå¤±æ•—ï¼š{e}")
    return None

# âœ… æå–éª¨æ¶ç‰¹å¾µ
def extract_features(frames):
    diffs = np.linalg.norm(np.diff(frames[:, :2], axis=0), axis=1)
    return {
        "avg_speed": round(float(np.mean(diffs)), 4),
        "max_speed": round(float(np.max(diffs)), 4)
    }

# âœ… ç”Ÿæˆæè¿°ï¼ˆå„ªå…ˆ Ollama â†’ Fallback æ¨¡æ¿ï¼‰
def get_description(action, features, model_name="qwen"):
    prompt = f"è«‹ç”¨ä¸­æ–‡å¯«1-2å¥å„ªé›…ä¸”å¯Œæœ‰è©©æ„çš„èˆè¹ˆè©•è«–ï¼Œæè¿°èˆè€…æ­£åœ¨è¡¨æ¼”ã€Œ{action}ã€ï¼Œå¹³å‡é€Ÿåº¦{features['avg_speed']}ï¼Œæœ€å¤§é€Ÿåº¦{features['max_speed']}ã€‚"
    text = ollama_generate(prompt, model_name)
    if text:
        return text
    return local_templates.get(action, f"èˆè€…æ­£åœ¨è¡¨æ¼” {action}ï¼Œå‹•ä½œæµæš¢è‡ªç„¶ã€‚")

# ğŸ”¥ è™•ç†å½±ç‰‡
for video in os.listdir(video_folder):
    if not video.endswith(".mp4"):
        continue

    print(f"\nğŸ¬ æ¸¬è©¦å½±ç‰‡ï¼š {video}")
    frames = []
    cap = cv2.VideoCapture(os.path.join(video_folder, video))

    while True:
        ret, frame = cap.read()
        if not ret:
            break
        rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
        results = pose.process(rgb)
        if results.pose_landmarks:
            landmarks = []
            for lm in results.pose_landmarks.landmark:
                landmarks += [lm.x, lm.y, lm.z, lm.visibility]
            frames.append(landmarks)
    cap.release()

    frames = np.array(frames)
    if len(frames) < time_steps and len(frames) > 0:
        pad = np.repeat(frames[-1][np.newaxis, :], time_steps - len(frames), axis=0)
        frames = np.vstack([frames, pad])

    if len(frames) >= time_steps:
        clip = frames[:time_steps].reshape(1, time_steps, 132)
        pred = model.predict(clip)
        predicted_action = reverse_dict[np.argmax(pred)]
        features = extract_features(frames[:time_steps])

        # ğŸ¨ ç”Ÿæˆè©©æ„æè¿°
        poetic_text = get_description(predicted_action, features, model_name="qwen")
        print(f"ğŸ¯ AI å‹•ä½œï¼š {predicted_action}ï¼ˆç½®ä¿¡åº¦ {np.max(pred):.2f}ï¼‰")
        print(f"ğŸ¨ Qwen è©©æ„æè¿°ï¼š {poetic_text}")
    else:
        print("âš ï¸ ç„¡æ³•é æ¸¬ï¼ˆå½±ç‰‡ç„¡éª¨æ¶ï¼‰")
